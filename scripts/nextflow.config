params {
  custom_config_version = 'master'
  custom_config_base = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
  rastools_dir = "/cobra/u/lamni/Software/RAStools"
  max_ras_ac = 5
  poseidon_exec_dir = "/r1/people/thiseas_christos_lamnidis/.local/bin/"
  n_ind_per_pop = 500
  chrom_length = 1000000
  num_chroms = 2000
  total_inds_per_pop = 500
}

try {
  includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
  System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

tower {
//  enabled = true
  accessToken = 'eyJ0aWQiOiAzNTk0fS42ZjFhOWFmYTFjYzNmOGM2N2Q2NTdiMmU2NWNlMmQ0MzBiYjNmN2Fj'
  workspaceId = '69890786810149'
}

profiles {
  var_sim {
  cleanup = false
    
  // Increase number of concurrent jobs to 24
    executor {
      queueSize = 120
    }
  }
  conda { process.conda = "$baseDir/environment.yml" }
  debug { process.beforeScript = 'echo $HOSTNAME' }
  docker {
    docker.enabled = true
    // Avoid this error:
    //   WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
    // Testing this in nf-core after discussion here https://github.com/nf-core/tools/pull/351
    // once this is established and works well, nextflow might implement this behavior as new default.
    docker.runOptions = '-u \$(id -u):\$(id -g)'
  }
  singularity {
    singularity.enabled = true
    singularity.autoMounts = true
  }
}

process {
  withName: xerxes_pairwise_ras {
    clusterOptions = { "-S /bin/bash -V -l h_vmem=${(task.memory.toGiga())}G,h=!(bionode01|bionode02|bionode03|bionode04|bionode05|bionode06|bionode27)" }
  }

  withName: xerxes_ras {
    clusterOptions = { "-S /bin/bash -V -l h_vmem=${(task.memory.toGiga())}G,h=!(bionode01|bionode02|bionode03|bionode04|bionode05|bionode06|bionode27)" }
  }

  withName: xerxes_f3 {
    clusterOptions = { "-S /bin/bash -V -l h_vmem=${(task.memory.toGiga())}G,h=!(bionode01|bionode02|bionode03|bionode04|bionode05|bionode06|bionode27)" }
  }

  withName: xerxes_f4 {
    clusterOptions = { "-S /bin/bash -V -l h_vmem=${(task.memory.toGiga())}G,h=!(bionode01|bionode02|bionode03|bionode04|bionode05|bionode06|bionode27)" }
  }

  withName: msprime {
    maxRetries = 2
    memory = { task.attempt == 3 ? 64.GB : task.attempt == 2 ? 48.GB : 32.GB }
    clusterOptions = { "-S /bin/bash -V -l h_vmem=${(task.memory.toGiga())}G,h=!(bionode01|bionode02|bionode03|bionode04|bionode05|bionode06|bionode27)" }
  }
}

// Load base.config by default for all pipelines
includeConfig 'base.config'

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
